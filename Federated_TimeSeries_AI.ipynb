{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"19J9s8TZ4w68yZBGZd9H5OQRsZ_GrylgQ","timestamp":1712946712558}],"authorship_tag":"ABX9TyO92frVNKvjg5vFUHJC0Uz0"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"pBe_-c_zaGzW","executionInfo":{"status":"ok","timestamp":1712947032974,"user_tz":240,"elapsed":1248,"user":{"displayName":"robert benjamin","userId":"12496057343112273971"}},"outputId":"929e9ee1-80ae-4813-f309-feef8eacdf72"},"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')\n","\n","import numpy as np\n","import random\n","import cv2\n","import os\n","from imutils import paths\n","from sklearn.model_selection import train_test_split\n","from sklearn.preprocessing import LabelBinarizer\n","from sklearn.model_selection import train_test_split\n","from sklearn.utils import shuffle\n","from sklearn.metrics import accuracy_score\n","\n","import tensorflow as tf\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import Conv2D\n","from tensorflow.keras.layers import MaxPooling2D\n","from tensorflow.keras.layers import Activation\n","from tensorflow.keras.layers import Flatten\n","from tensorflow.keras.layers import Dense\n","from tensorflow.keras.optimizers import SGD\n","from tensorflow.keras import backend as K\n","\n","# cnn model\n","from numpy import mean\n","from numpy import std\n","from numpy import dstack\n","from pandas import read_csv\n","from matplotlib import pyplot\n","\n","\n","#from fl_mnist_implementation_tutorial_utils import *"]},{"cell_type":"code","source":["import pandas as pd\n","import numpy as np\n","\n","\n","\n","from sklearn import model_selection\n","from IPython.display import display, HTML\n","\n","from tensorflow.keras import Model\n","from tensorflow.keras.optimizers import Adam\n","from tensorflow.keras.layers import Dense, Conv1D, MaxPool1D, Dropout, Flatten\n","from tensorflow.keras.losses import binary_crossentropy\n","import tensorflow as tf\n","import json\n","import math\n","import os\n","import cv2\n","from PIL import Image\n","import numpy as np\n","from keras import layers\n","from keras.applications import DenseNet201\n","from keras.callbacks import Callback, ModelCheckpoint, ReduceLROnPlateau, TensorBoard\n","from keras.preprocessing.image import ImageDataGenerator\n","from keras.utils import to_categorical\n","from keras.models import Sequential\n","from keras.optimizers import Adam\n","import matplotlib.pyplot as plt\n","import pandas as pd\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import cohen_kappa_score, accuracy_score\n","import scipy\n","from tqdm import tqdm\n","import tensorflow as tf\n","from keras import backend as K\n","from pathlib import Path # help manage f\n","import gc\n","from functools import partial\n","from sklearn import metrics\n","import itertools\n","import seaborn as sns\n","from keras.models import Model\n","from keras.optimizers import Adam\n","from keras.applications.resnet50 import ResNet50, preprocess_input\n","from keras.preprocessing.image import ImageDataGenerator\n","from keras.callbacks import ModelCheckpoint, EarlyStopping\n","from keras.layers import Dense, Dropout, Flatten\n","from pathlib import Path\n","import numpy as np\n","from matplotlib import pyplot as plt\n","\n","from keras.applications.vgg16 import VGG16, preprocess_input\n","from keras.preprocessing.image import load_img, img_to_array\n","from sklearn.preprocessing import LabelBinarizer"],"metadata":{"id":"-epFopeNmtT5","executionInfo":{"status":"ok","timestamp":1712947146103,"user_tz":240,"elapsed":418,"user":{"displayName":"robert benjamin","userId":"12496057343112273971"}}},"execution_count":5,"outputs":[]},{"cell_type":"code","source":["# load a single file as a numpy array\n","def load_file(filepath):\n","\tdataframe = read_csv(filepath, header=None, delim_whitespace=True)\n","\treturn dataframe.values\n","\n","# load a list of files and return as a 3d numpy array\n","def load_group(filenames, prefix=''):\n","\tloaded = list()\n","\tfor name in filenames:\n","\t\tdata = load_file(prefix + name)\n","\t\tloaded.append(data)\n","\t# stack group so that features are the 3rd dimension\n","\tloaded = dstack(loaded)\n","\treturn loaded\n","\n","# load a dataset group, such as train or test\n","def load_dataset_group(group, prefix=''):\n","\tfilepath = prefix + group + '/Inertial Signals/'\n","\t# load all 9 files as a single array\n","\tfilenames = list()\n","\t# total acceleration\n","\tfilenames += ['total_acc_x_'+group+'.txt', 'total_acc_y_'+group+'.txt', 'total_acc_z_'+group+'.txt']\n","\t# body acceleration\n","\tfilenames += ['body_acc_x_'+group+'.txt', 'body_acc_y_'+group+'.txt', 'body_acc_z_'+group+'.txt']\n","\t# body gyroscope\n","\tfilenames += ['body_gyro_x_'+group+'.txt', 'body_gyro_y_'+group+'.txt', 'body_gyro_z_'+group+'.txt']\n","\t# load input data\n","\tX = load_group(filenames, filepath)\n","\t# load class output\n","\ty = load_file(prefix + group + '/y_'+group+'.txt')\n","\treturn X, y\n","\n","# load the dataset, returns train and test X and y elements\n","def load_dataset(prefix='/content/drive/My Drive/Datasets/TimeSeries/UCI HAR Dataset/'):\n","\t# load all train\n","\t#trainX, trainy = load_dataset_group('train', prefix + 'HARDataset/')\n","    trainX, trainy = load_dataset_group('train', prefix)\n","    print(trainX.shape, trainy.shape)\n","\t# load all test\n","\t#testX, testy = load_dataset_group('test', prefix + 'HARDataset/')\n","    testX, testy = load_dataset_group('test', prefix )\n","    print(testX.shape, testy.shape)\n","\t# zero-offset class values\n","    trainy = trainy - 1\n","    testy = testy - 1\n","\t# one hot encode y\n","    trainy = to_categorical(trainy)\n","    testy = to_categorical(testy)\n","    print(trainX.shape, trainy.shape, testX.shape, testy.shape)\n","    return trainX, trainy, testX, testy\n","\n","# fit and evaluate a model\n","def evaluate_model(trainX, trainy, testX, testy):\n","\tverbose, epochs, batch_size = 0, 10, 32\n","\tn_timesteps, n_features, n_outputs = trainX.shape[1], trainX.shape[2], trainy.shape[1]\n","\tmodel = Sequential()\n","\tmodel.add(Conv1D(filters=64, kernel_size=3, activation='relu', input_shape=(n_timesteps,n_features)))\n","\tmodel.add(Conv1D(filters=64, kernel_size=3, activation='relu'))\n","\tmodel.add(Dropout(0.5))\n","\tmodel.add(MaxPool1D(pool_size=2))\n","\tmodel.add(Flatten())\n","\tmodel.add(Dense(100, activation='relu'))\n","\tmodel.add(Dense(n_outputs, activation='softmax'))\n","\tmodel.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n","\t# fit network\n","\tmodel.fit(trainX, trainy, epochs=epochs, batch_size=batch_size, verbose=verbose)\n","\t# evaluate model\n","\t_, accuracy = model.evaluate(testX, testy, batch_size=batch_size, verbose=0)\n","\treturn accuracy\n","\n","# summarize scores\n","def summarize_results(scores):\n","\tprint(scores)\n","\tm, s = mean(scores), std(scores)\n","\tprint('Accuracy: %.3f%% (+/-%.3f)' % (m, s))\n","\n","# run an experiment\n","def run_experiment(repeats=10):\n","\t# load data\n","\ttrainX, trainy, testX, testy = load_dataset()\n","\t# repeat experiment\n","\tscores = list()\n","\tfor r in range(repeats):\n","\t\tscore = evaluate_model(trainX, trainy, testX, testy)\n","\t\tscore = score * 100.0\n","\t\tprint('>#%d: %.3f' % (r+1, score))\n","\t\tscores.append(score)\n","\t# summarize results\n","\tsummarize_results(scores)\n","\n","# run the experiment\n","run_experiment()\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"mqfGZUIerfTq","executionInfo":{"status":"ok","timestamp":1712948386251,"user_tz":240,"elapsed":662460,"user":{"displayName":"robert benjamin","userId":"12496057343112273971"}},"outputId":"9de5c44d-0ace-4e6b-899a-776da2807d70"},"execution_count":7,"outputs":[{"output_type":"stream","name":"stdout","text":["(7352, 128, 9) (7352, 1)\n","(2947, 128, 9) (2947, 1)\n","(7352, 128, 9) (7352, 6) (2947, 128, 9) (2947, 6)\n",">#1: 89.888\n",">#2: 92.026\n",">#3: 87.547\n",">#4: 92.195\n",">#5: 90.126\n",">#6: 90.058\n",">#7: 91.008\n",">#8: 90.635\n",">#9: 90.499\n",">#10: 91.924\n","[89.88802433013916, 92.02578663825989, 87.54665851593018, 92.19545125961304, 90.12554883956909, 90.05768299102783, 91.00780487060547, 90.63454270362854, 90.49881100654602, 91.923987865448]\n","Accuracy: 90.590% (+/-1.298)\n"]}]},{"cell_type":"code","source":["'''\n","#----Create numpy array of 'zeros' and 'ones' for labelling benign/malignant images resp.\n","benign_train_label = np.zeros(len(benign_train))\n","malign_train_label = np.ones(len(malign_train))\n","#benign_train_label = (-1)*benign_train_label2\n","\n","X_data = np.concatenate((benign_train, malign_train), axis = 0)\n","Y_data = np.concatenate((benign_train_label, malign_train_label), axis = 0)\n","#X_test = np.concatenate((benign_valid, malign_valid), axis = 0)\n","#Y_test = np.concatenate((benign_test_label, malign_test_label), axis = 0)\n","\n","s = np.arange(X_data.shape[0])\n","np.random.shuffle(s)\n","X_data = X_data[s] #Re-index\n","Y_data = Y_data[s]\n","\n","#binarize the labels\n","#lb = LabelBinarizer()\n","#label_list = lb.fit_transform(Y_data)\n","\n","#split data into training and test set\n","X_train, X_test, y_train, y_test = train_test_split(X_data,\n","                                                    Y_data,\n","                                                    test_size=0.1,\n","                                                    random_state=42)\n","\n","'''"],"metadata":{"id":"Zqp1oTYD0oGZ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["X_train, y_train, X_test, y_test = load_dataset()\n","print(X_train.shape)\n","\n","lb = LabelBinarizer()\n","labely = lb.fit_transform(y_test)\n","labelx = lb.fit_transform(y_train)\n","\n","print(labely)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"1M17WU2cvZoo","executionInfo":{"status":"ok","timestamp":1712953191489,"user_tz":240,"elapsed":9717,"user":{"displayName":"robert benjamin","userId":"12496057343112273971"}},"outputId":"5ca3b650-e0ee-4900-967f-2a276e502e84"},"execution_count":46,"outputs":[{"output_type":"stream","name":"stdout","text":["(7352, 128, 9) (7352, 1)\n","(2947, 128, 9) (2947, 1)\n","(7352, 128, 9) (7352, 6) (2947, 128, 9) (2947, 6)\n","(7352, 128, 9)\n","[[0 0 0 0 1 0]\n"," [0 0 0 0 1 0]\n"," [0 0 0 0 1 0]\n"," ...\n"," [0 1 0 0 0 0]\n"," [0 1 0 0 0 0]\n"," [0 1 0 0 0 0]]\n"]}]},{"cell_type":"markdown","source":["Federated Members (clients) as Data Shards"],"metadata":{"id":"IA6lUWFW5GcV"}},{"cell_type":"code","source":["def create_clients(image_list, label_list, num_clients=2, initial='clients'):\n","    ''' return: a dictionary with keys clients' names and value as\n","                data shards - tuple of images and label lists.\n","        args:\n","            image_list: a list of numpy arrays of training images\n","            label_list:a list of binarized labels for each image\n","            num_client: number of fedrated members (clients)\n","            initials: the clients'name prefix, e.g, clients_1\n","\n","    '''\n","\n","    #create a list of client names\n","    client_names = ['{}_{}'.format(initial, i+1) for i in range(num_clients)]\n","    print(client_names)\n","    #randomize the data\n","    data = list(zip(image_list, label_list))\n","    random.shuffle(data)\n","\n","    #shard data and place at each client\n","    size = len(data)//num_clients\n","    shards = [data[i:i + size] for i in range(0, size*num_clients, size)]\n","\n","    #number of clients must equal number of shards\n","    assert(len(shards) == len(client_names))\n","\n","    return {client_names[i] : shards[i] for i in range(len(client_names))}\n","\n","#create clients\n","clients = create_clients(X_train, y_train, num_clients=2, initial='client')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"RUmmvrJb31e8","executionInfo":{"status":"ok","timestamp":1712953283902,"user_tz":240,"elapsed":129,"user":{"displayName":"robert benjamin","userId":"12496057343112273971"}},"outputId":"623b733e-737b-46cf-ca8f-435dab5f2c44"},"execution_count":53,"outputs":[{"output_type":"stream","name":"stdout","text":["['client_1', 'client_2']\n"]}]},{"cell_type":"markdown","source":["Processing and batching clientsâ€™ and test data"],"metadata":{"id":"zYSUvdV_4-pI"}},{"cell_type":"code","source":["def batch_data(data_shard, bs=32):\n","    '''Takes in a clients data shard and create a tfds object off it\n","    args:\n","        shard: a data, label constituting a client's data shard\n","        bs:batch size\n","    return:\n","        tfds object'''\n","    #seperate shard into data and labels lists\n","    data, label = zip(*data_shard)\n","    dataset = tf.data.Dataset.from_tensor_slices((list(data), list(label)))\n","    return dataset.shuffle(len(label)).batch(bs)\n","\n","\n","#process and batch the training data for each client\n","clients_batched = dict()\n","for (client_name, data) in clients.items():\n","    clients_batched[client_name] = batch_data(data)\n","\n","#process and batch the test set\n","test_batched = tf.data.Dataset.from_tensor_slices((X_test, y_test)).batch(len(y_test))"],"metadata":{"id":"yfjxEjGf4Lg6","executionInfo":{"status":"ok","timestamp":1712953291660,"user_tz":240,"elapsed":3046,"user":{"displayName":"robert benjamin","userId":"12496057343112273971"}}},"execution_count":54,"outputs":[]},{"cell_type":"markdown","source":["Creating the Multi Layer Perceptron (MLP) model"],"metadata":{"id":"9IFtD7wF45lk"}},{"cell_type":"code","source":["from keras.optimizers import SGD\n","class SimpleMLP:\n","    @staticmethod\n","    def build(n_timesteps,n_features, n_outputs):\n","        #verbose, epochs, batch_size = 0, 10, 32\n","\t      #n_timesteps, n_features, n_outputs = trainX.shape[1], trainX.shape[2], trainy.shape[1] =\n","\t      model = Sequential()\n","\t      model.add(Conv1D(filters=64, kernel_size=3, activation='relu', input_shape=(n_timesteps,n_features)))\n","\t      model.add(Conv1D(filters=64, kernel_size=3, activation='relu'))\n","\t      model.add(Dropout(0.5))\n","\t      model.add(MaxPool1D(pool_size=2))\n","\t      model.add(Flatten())\n","\t      model.add(Dense(100, activation='relu'))\n","\t      model.add(Dense(n_outputs, activation='softmax'))\n","\t      #model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n","\t      # fit network\n","\t      #model.fit(trainX, trainy, epochs=epochs, batch_size=batch_size, verbose=verbose)\n","\t      # evaluate model\n","\t      #_, accuracy = model.evaluate(testX, testy, batch_size=batch_size, verbose=0)\n","\t      return model\n","\n","\n","\n","\n","time_model = evaluate_model(X_train, y_train, X_test, y_test)\n","time_model.summary()\n","lr = 0.01\n","comms_round = 100\n","loss='categorical_crossentropy'\n","metrics = ['accuracy']\n","optimizer = tf.keras.optimizers.legacy.Adam(learning_rate=lr)\n","verbose, epochs, batch_size = 0, 10, 32"],"metadata":{"id":"vZMNl0HK4dPz","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1712953224422,"user_tz":240,"elapsed":289,"user":{"displayName":"robert benjamin","userId":"12496057343112273971"}},"outputId":"fa59ded6-49b1-46a2-eb6c-45ee62b0fe36"},"execution_count":49,"outputs":[{"output_type":"stream","name":"stdout","text":["Model: \"sequential\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," conv1d (Conv1D)             (None, 126, 64)           1792      \n","                                                                 \n"," conv1d_1 (Conv1D)           (None, 124, 64)           12352     \n","                                                                 \n"," dropout (Dropout)           (None, 124, 64)           0         \n","                                                                 \n"," max_pooling1d (MaxPooling1  (None, 62, 64)            0         \n"," D)                                                              \n","                                                                 \n"," flatten (Flatten)           (None, 3968)              0         \n","                                                                 \n"," dense (Dense)               (None, 100)               396900    \n","                                                                 \n"," dense_1 (Dense)             (None, 6)                 606       \n","                                                                 \n","=================================================================\n","Total params: 411650 (1.57 MB)\n","Trainable params: 411650 (1.57 MB)\n","Non-trainable params: 0 (0.00 Byte)\n","_________________________________________________________________\n"]}]},{"cell_type":"markdown","source":["Model Aggregation (Federated Averaging)"],"metadata":{"id":"8YR-uDCQ4zYP"}},{"cell_type":"code","source":["def weight_scalling_factor(clients_trn_data, client_name):\n","    client_names = list(clients_trn_data.keys())\n","    #get the bs\n","    bs = list(clients_trn_data[client_name])[0][0].shape[0]\n","    #first calculate the total training data points across clinets\n","    global_count = sum([tf.data.experimental.cardinality(clients_trn_data[client_name]).numpy() for client_name in client_names])*bs\n","    # get the total number of data points held by a client\n","    local_count = tf.data.experimental.cardinality(clients_trn_data[client_name]).numpy()*bs\n","    return local_count/global_count\n","\n","\n","def scale_model_weights(weight, scalar):\n","    '''function for scaling a models weights'''\n","    weight_final = []\n","    steps = len(weight)\n","    for i in range(steps):\n","        weight_final.append(scalar * weight[i])\n","    return weight_final\n","\n","\n","\n","def sum_scaled_weights(scaled_weight_list):\n","    '''Return the sum of the listed scaled weights. The is equivalent to scaled avg of the weights'''\n","    avg_grad = list()\n","    #get the average grad accross all client gradients\n","    for grad_list_tuple in zip(*scaled_weight_list):\n","        layer_mean = tf.math.reduce_sum(grad_list_tuple, axis=0)\n","        avg_grad.append(layer_mean)\n","\n","    return avg_grad\n","\n","\n"],"metadata":{"id":"FQAu4A8Q4reH","executionInfo":{"status":"ok","timestamp":1712953228826,"user_tz":240,"elapsed":120,"user":{"displayName":"robert benjamin","userId":"12496057343112273971"}}},"execution_count":50,"outputs":[]},{"cell_type":"code","source":["\n","\n","def test_model(X_test,y_test,  model, comm_round):\n","    #cce = tf.keras.losses.SparseCategoricalCrossentropy()\n","    cce = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n","    logits = model.predict(X_test, batch_size=100)\n","    print(\"Logits shape:\", logits.shape)\n","    print(\"y_test shape:\", y_test.shape)\n","    #logits = (model.predict(X_test) > 0.5).astype(\"float\")\n","    #logits = model.predict(X_test)\n","    #labels = tf.reshape(y_test, (-1, 6))  # Reshape label_list to match logits shape\n","    loss = cce(y_test, logits)\n","    #print(logits)\n","    #loss = cce(y_test)\n","    #acc = accuracy_score(tf.argmax(logits, axis=1), tf.argmax(y_test, axis = 1))\n","    acc = accuracy_score(tf.argmax(logits, axis=1), y_test)\n","    print('comm_round: {} | global_acc: {:.3%} | global_loss: {}'.format(comm_round, acc, loss))\n","    return acc, loss"],"metadata":{"id":"v4bn50WKw_Yg","executionInfo":{"status":"ok","timestamp":1712954358928,"user_tz":240,"elapsed":134,"user":{"displayName":"robert benjamin","userId":"12496057343112273971"}}},"execution_count":79,"outputs":[]},{"cell_type":"markdown","source":["Federated Model Training\n"],"metadata":{"id":"-D7mgyM95O3j"}},{"cell_type":"code","source":["#initialize global model\n","smlp_global = SimpleMLP()\n","global_model = smlp_global.build(128, 9, 6)\n","\n","global_acc_list = []\n","global_loss_list = []\n","\n","#commence global training loop\n","for comm_round in range(comms_round):\n","\n","    # get the global model's weights - will serve as the initial weights for all local models\n","    global_weights = global_model.get_weights()\n","\n","    #initial list to collect local model weights after scalling\n","    scaled_local_weight_list = list()\n","\n","    #randomize client data - using keys\n","    client_names= list(clients_batched.keys())\n","    random.shuffle(client_names)\n","\n","    #loop through each client and create new local model\n","    for client in client_names:\n","        smlp_local = SimpleMLP()\n","        local_model = smlp_local.build(128, 9, 6)\n","        local_model.compile(loss=loss,\n","                      optimizer=optimizer,\n","                      metrics=metrics)\n","\n","        #set local model weight to the weight of the global model\n","        local_model.set_weights(global_weights)\n","\n","        #fit local model with client's data\n","        local_model.fit(clients_batched[client], epochs=1, verbose=0)\n","\n","        #scale the model weights and add to list\n","        scaling_factor = weight_scalling_factor(clients_batched, client)\n","        scaled_weights = scale_model_weights(local_model.get_weights(), scaling_factor)\n","        scaled_local_weight_list.append(scaled_weights)\n","\n","        #clear session to free memory after each communication round\n","        K.clear_session()\n","\n","    #to get the average over all the local model, we simply take the sum of the scaled weights\n","    average_weights = sum_scaled_weights(scaled_local_weight_list)\n","\n","    #update global model\n","    global_model.set_weights(average_weights)\n","\n","    #test global model and print out metrics after each communications round\n","    for(X_test, y_test) in test_batched:\n","        global_acc, global_loss = test_model(X_test, y_test, global_model, comm_round)\n","\n","\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":443},"id":"Vj4NeMXV5T1_","executionInfo":{"status":"error","timestamp":1712954368432,"user_tz":240,"elapsed":6956,"user":{"displayName":"robert benjamin","userId":"12496057343112273971"}},"outputId":"7a5051f2-698f-4a47-980d-c0b9b90e0328"},"execution_count":80,"outputs":[{"output_type":"stream","name":"stdout","text":["30/30 [==============================] - 0s 13ms/step\n","Logits shape: (2947, 6)\n","y_test shape: (2947, 6)\n"]},{"output_type":"error","ename":"ValueError","evalue":"`labels.shape` must equal `logits.shape` except for the last dimension. Received: labels.shape=(17682,) and logits.shape=(2947, 6)","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","\u001b[0;32m<ipython-input-80-789d77d1ae42>\u001b[0m in \u001b[0;36m<cell line: 9>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     49\u001b[0m     \u001b[0;31m#test global model and print out metrics after each communications round\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m     \u001b[0;32mfor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtest_batched\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m         \u001b[0mglobal_acc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mglobal_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mglobal_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcomm_round\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-79-4466201c38f7>\u001b[0m in \u001b[0;36mtest_model\u001b[0;34m(X_test, y_test, model, comm_round)\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0;31m#logits = model.predict(X_test)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0;31m#labels = tf.reshape(y_test, (-1, 6))  # Reshape label_list to match logits shape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcce\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogits\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m     \u001b[0;31m#print(logits)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0;31m#loss = cce(y_test)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/losses.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, y_true, y_pred, sample_weight)\u001b[0m\n\u001b[1;32m    141\u001b[0m                 )\n\u001b[1;32m    142\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 143\u001b[0;31m             \u001b[0mlosses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcall_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    144\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    145\u001b[0m             \u001b[0min_mask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlosses_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_mask\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/losses.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, y_true, y_pred)\u001b[0m\n\u001b[1;32m    268\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__internal__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrol_status_ctx\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    269\u001b[0m         )\n\u001b[0;32m--> 270\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mag_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fn_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    271\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    272\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_config\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/util/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    151\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 153\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    154\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    155\u001b[0m       \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/losses.py\u001b[0m in \u001b[0;36msparse_categorical_crossentropy\u001b[0;34m(y_true, y_pred, from_logits, axis, ignore_class)\u001b[0m\n\u001b[1;32m   2452\u001b[0m         \u001b[0mSparse\u001b[0m \u001b[0mcategorical\u001b[0m \u001b[0mcrossentropy\u001b[0m \u001b[0mloss\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2453\u001b[0m     \"\"\"\n\u001b[0;32m-> 2454\u001b[0;31m     return backend.sparse_categorical_crossentropy(\n\u001b[0m\u001b[1;32m   2455\u001b[0m         \u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2456\u001b[0m         \u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/backend.py\u001b[0m in \u001b[0;36msparse_categorical_crossentropy\u001b[0;34m(target, output, from_logits, axis, ignore_class)\u001b[0m\n\u001b[1;32m   5777\u001b[0m             )\n\u001b[1;32m   5778\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 5779\u001b[0;31m         res = tf.nn.sparse_softmax_cross_entropy_with_logits(\n\u001b[0m\u001b[1;32m   5780\u001b[0m             \u001b[0mlabels\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogits\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5781\u001b[0m         )\n","\u001b[0;31mValueError\u001b[0m: `labels.shape` must equal `logits.shape` except for the last dimension. Received: labels.shape=(17682,) and logits.shape=(2947, 6)"]}]},{"cell_type":"markdown","source":["SGD Vs Federated Averaging"],"metadata":{"id":"_fKfnBDg6Jsu"}},{"cell_type":"code","source":["SGD_dataset = tf.data.Dataset.from_tensor_slices((X_train, y_train)).shuffle(len(y_train)).batch(320)\n","smlp_SGD = SimpleMLP()\n","SGD_model = smlp_SGD.build(784, 10)\n","\n","SGD_model.compile(loss=loss,\n","              optimizer=optimizer,\n","              metrics=metrics)\n","\n","# fit the SGD training data to model\n","_ = SGD_model.fit(SGD_dataset, epochs=100, verbose=0)\n","\n","#test the SGD global model and print out metrics\n","for(X_test, Y_test) in test_batched:\n","        SGD_acc, SGD_loss = test_model(X_test, Y_test, SGD_model, 1)\n"],"metadata":{"id":"I3SBuSm75b1a"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":[],"metadata":{"id":"fkMHpjcF6VA5"}},{"cell_type":"code","source":["def non_iid_x(image_list, label_list, x=1, num_intraclass_clients=10):\n","        ''' creates x non_IID clients\n","        args:\n","            image_list: python list of images or data points\n","            label_list: python list of labels\n","            x: none IID severity, 1 means each client will only have one class of data\n","            num_intraclass_client: number of sub-client to be created from each none IID class,\n","            e.g for x=1, we could create 10 further clients by splitting each class into 10\n","\n","        return - dictionary\n","            keys - clients's name,\n","            value - client's non iid 1 data shard (as tuple list of images and labels) '''\n","\n","        non_iid_x_clients = dict()\n","\n","        #create unique label list and shuffle\n","        unique_labels = np.unique(np.array(label_list))\n","        random.shuffle(unique_labels)\n","\n","        #create sub label lists based on x\n","        sub_lab_list = [unique_labels[i:i + x] for i in range(0, len(unique_labels), x)]\n","\n","        for item in sub_lab_list:\n","            class_data = [(image, label) for (image, label) in zip(image_list, label_list) if label in item]\n","\n","            #decouple tuple list into seperate image and label lists\n","            images, labels = zip(*class_data)\n","\n","            # create formated client initials\n","            initial = ''\n","            for lab in item:\n","                initial = initial + lab + '_'\n","\n","            #create num_intraclass_clients clients from the class\n","            intraclass_clients = create_clients(list(images), list(labels), num_intraclass_clients, initial)\n","\n","            #append intraclass clients to main clients'dict\n","            non_iid_x_clients.update(intraclass_clients)\n","\n","        return non_iid_x_clients"],"metadata":{"id":"-2m6bZ5k6VR5"},"execution_count":null,"outputs":[]}]}